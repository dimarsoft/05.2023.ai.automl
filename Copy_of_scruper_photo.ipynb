{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dimarsoft/05.2023.ai.automl/blob/main/Copy_of_scruper_photo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BDmwlhUrTIcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Кастомный скрапер писался для выгрузки в colab фото с первой html-страницы Яндекс.картинки. Но может тянуть изображения и с некоторых других сайтов. Маркетплейсы и другие агрегаторы изображений ставят защиту или загружают фавиконы или угрозы."
      ],
      "metadata": {
        "id": "2nS3ak8DTwj0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**КАК УСТРОЕН**\n",
        "\n",
        "Работает по поисковому запросу в строке ресурса. В примере ниже загружены фото по запросу на русском языке \"автомобиль Lada\". Можно писать на разных языках сколько угодно слов поиска (в разумных пределах, конечно).\n",
        "\n",
        "С учетом запроса ресурс сгенерирует адресную строку, которую надо скопировать и вставить в код.\n",
        "\n",
        "С первой html-страницы можно награбить в районе 200 фото. Если нобходимо грабить вторую, третью и т.д. страницы, достаточно внести соответствующие поправки руками в ссылку.\n",
        "\n",
        "По ходу работы скрапер прорежает временную папку colab, удаляя некондционные файлы, и дает статистику по количеству оставшихся.\n",
        "\n",
        "Через 3-4 запуска рекомендуется перегрузить скрапер - Яндекс тоже может выставить защиту.\n",
        "\n",
        "**ЧТО ДЕЛАЕТ**\n",
        "\n",
        "- тянет все форматы, которые сочтет изображением\n",
        "- выбирает уникальные ссылки для скачивания\n",
        "- игнорирует ошибки загрузки после первой попытки обращения к сайту хранения картинки\n",
        "- игнорирует ссылки больше 230 символов (они, как правило, выдают ошибки при скачивании)\n",
        "- раскладывает файлы по трем папкам в завсимости от размера (Big_over_2000px, Middle_500-2000px, Small_less_500px).\n",
        "- В названиях файлов также указывается размер картинки и далее через нижнюю несколько первых символов адреса сайта (например, \"2048x1536_avtomob.jpg\").\n",
        "- Папки с размерами зиппуются и автоматом скачиваются в папку загрузки компьютера (среда Windows, остальные операционки не пробовала).\n",
        "\n"
      ],
      "metadata": {
        "id": "fXKmfrt7UfIK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oO_DqPfOdIVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KaSqHJd8Sv2K"
      },
      "outputs": [],
      "source": [
        "!pip install transliterate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import re\n",
        "import os\n",
        "import zipfile\n",
        "import random\n",
        "import time\n",
        "import shutil\n",
        "import urllib.parse\n",
        "import urllib.request\n",
        "from transliterate import translit\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.request import Request, urlopen\n",
        "from urllib.error import HTTPError\n",
        "from PIL import Image\n",
        "\n",
        "# Тут скачивается html-код сайта - soap ----------------------------------------\n",
        "# Он дербанится на список ссылок - links\n",
        "\n",
        "    '''\n",
        "    Сюда вставляется url\n",
        "    в данном случае запрос \"автомобиль Lada\"\n",
        "\n",
        "    url = 'https://yandex.ru/images/search?text=%D0%B0%D0%B2%D1%82%D0%BE%D0%BC%D0%BE%D0%B1%D0%B8%D0%BB%D1%8C%20Lada'\n",
        "\n",
        "    '''\n",
        "\n",
        "parsed_url = urllib.parse.urlparse(url)\n",
        "query_dict = urllib.parse.parse_qs(parsed_url.query)\n",
        "\n",
        "# с get должно работать даже без ключа\n",
        "text = urllib.parse.unquote(query_dict.get('text', [''])[0])\n",
        "perem_rus = text.replace(' ', '_').title()\n",
        "perem_rus = translit(text, 'ru', reversed=True)\n",
        "perem_eng = perem_rus.replace(' ', '_').title()\n",
        "\n",
        "if parsed_url.netloc != '':\n",
        "  perem_rus = parsed_url.netloc.split('.')[0]\n",
        "  perem_eng = translit(perem_rus, 'ru', reversed=True).replace(' ', '_').title()\n",
        "else:\n",
        "  query_dict = urllib.parse.parse_qs(parsed_url.query)\n",
        "  text = urllib.parse.unquote(query_dict.get('text', [''])[0])\n",
        "  perem_rus = text.replace(' ', '_').title()\n",
        "  perem_eng = translit(perem_rus, 'ru', reversed=True).replace(' ', '_').title()\n",
        "\n",
        "# Скачиваем html-код со ссылками -----------------------------------------------\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "#soup\n",
        "\n",
        "spis_format = re.findall(r'[\\'\"](https[^\\'\" >]+\\.(?:jpg|jpeg|gif|png))', str(soup))\n",
        "unique_spis = set(spis_format)\n",
        "formatted_spis = [f'{line}' for line in unique_spis]\n",
        "links = formatted_spis\n",
        "kol_all = len(links)\n",
        "\n",
        "with open(f'Photo_{perem_eng}.txt', 'w') as f:\n",
        "  f.write('\\n'.join(links))\n",
        "\n",
        "# Закачиваем ссылки img\n",
        "# Тут ограничение по количеству попыток и времени коннекта с сервером-----------\n",
        "\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "# создаем папку и динамический path, чтобы он постоянно мог меняться------------\n",
        "save_dir = f'Folder_{perem_eng}'\n",
        "if not os.path.isdir(save_dir):\n",
        "  os.makedirs(save_dir)\n",
        "path = os.path.join('/content', save_dir)\n",
        "if not os.path.exists(path):\n",
        "    os.makedirs(path)\n",
        "\n",
        "for i, ss in enumerate(links): # x это links\n",
        "  ss = ss.strip()\n",
        "  image_name = f'{ss.split(\"/\")[-1]}'\n",
        "  if len(image_name) <= 230:\n",
        "    image_path = os.path.join(save_dir, image_name)\n",
        "    # 1 - количество попыток соединения с сервером, если тот не отвечает\n",
        "    for attempts in range(1):\n",
        "      try:\n",
        "        # 2 - количество секунд на каждую попытку\n",
        "        response = requests.get(ss, timeout=2)\n",
        "        with open(image_path, 'wb') as f:\n",
        "          f.write(response.content)\n",
        "        print(f'Успешно №{i+1} из {kol_all}: {image_name} from {ss}')\n",
        "        #print(f'Успешно №{i+1}: {image_name} from {ss}')\n",
        "        break # если загрузка прошла успешно, прерываем цикл попыток\n",
        "      except (requests.exceptions.RequestException, requests.exceptions.Timeout) as e:\n",
        "        print(f'Ошибка загрузки картинки с {ss} на попытке {attempts+1}: {e}')\n",
        "        time.sleep(10) # пауза перед следующей попыткой\n",
        "  else:\n",
        "    print(f'Ссылка {ss} больше 230 символов')\n",
        "\n",
        "t = int(time.time() - start_time)\n",
        "print('\\nКачалось {:02d} часов {:02d} минут {:02d} секунд'.format(t//3600, (t%3600)//60, t%60))\n",
        "\n",
        "print()\n",
        "print(f'Количество ссылок: \\nизначально - {len(spis_format)} \\nуникальных - {kol_all}')\n",
        "print()\n",
        "\n",
        "num_files = len([file for file in os.listdir(path) if os.path.isfile(os.path.join(path, file))])\n",
        "print(f'\\nВ папке \"{perem_eng}\" лежит {num_files} файлов')"
      ],
      "metadata": {
        "id": "rMRHNdmKTA_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ОБРАБОТКА ПАПКИ С ФАЙЛАМИ\n",
        "\n",
        "# Проверяем, все ли файлы = img, переименовываем в соотв с размером ------------\n",
        "files_list = []\n",
        "for file in os.listdir(path):\n",
        "  file_path = os.path.join(path, file)\n",
        "  files_list.append(file_path)\n",
        "\n",
        "i = 1\n",
        "for file in files_list:\n",
        "    try:\n",
        "        with Image.open(file) as img:\n",
        "            width, height = img.size\n",
        "            filename, ext = os.path.splitext(os.path.basename(file))\n",
        "            # тут я ограничиваю имя файла 20 символами после {width}x{height}_\n",
        "            new_filename = f'{width}x{height}_{filename[:16-len(str(width))-len(str(height))-1].strip(\"_\")}{ext}' if len(filename) > 20 else f'{width}x{height}_{filename}{ext}'\n",
        "            new_file = os.path.join(path, new_filename)\n",
        "            os.rename(file, new_file)\n",
        "            files_list[i-1] = new_file  # Обновляем список файлов с новым именами\n",
        "            #print(f'{new_filename}')\n",
        "            i += 1\n",
        "    except Exception as e:\n",
        "        #print(f\"WARNING: Файл {file} не является изображением или поврежден ({str(e)})\")\n",
        "        continue\n",
        "\n",
        "# Удаляем некондиционные файлы -------------------------------------------------\n",
        "num_deleted = 0\n",
        "\n",
        "for file in os.listdir(path):\n",
        "  if not re.match(r'^\\d+x\\d+_', file):\n",
        "    file_path = os.path.join(path, file)\n",
        "    os.remove(file_path)\n",
        "    num_deleted += 1\n",
        "    print(f'\\nУдалено {num_deleted} файлов')\n",
        "print(f'\\nВ папке \"{perem_eng}\" осталось {num_files - num_deleted} файлов')\n",
        "\n",
        "\n",
        "# Копируем файлы в новую папку по условию --------------------------------------\n",
        "new_path = os.path.join('/content', perem_eng)\n",
        "paps = ['Small_less_500px', 'Middle_500-2000px', 'Big_over_2000px']\n",
        "# создаем три подпапки в новой папке, куда копировать\n",
        "for p in paps:\n",
        "    os.makedirs(os.path.join(new_path, p), exist_ok=True)\n",
        "\n",
        "for file in os.listdir(path):\n",
        "    match = re.match(r'^(\\d+)x(\\d+)_.*$', file)\n",
        "    if match:\n",
        "        width = int(match.group(1))\n",
        "        height = int(match.group(2))\n",
        "\n",
        "        # переместить файл в нужную папку или удалить---------------------------\n",
        "        if width > 4000 or height > 4000:\n",
        "            os.remove(os.path.join(path, file))\n",
        "        elif width < 500 or height < 500:\n",
        "            shutil.copy(os.path.join(path, file), os.path.join(new_path, 'Small_less_500px'))\n",
        "        elif width >= 500 and width <= 2000 and height >= 500 and height <= 2000:\n",
        "            shutil.copy(os.path.join(path, file), os.path.join(new_path, 'Middle_500-2000px'))\n",
        "        elif width > 2000 or height > 2000:\n",
        "            shutil.copy(os.path.join(path, file), os.path.join(new_path, 'Big_over_2000px'))\n",
        "\n",
        "# Зазиповать всю папку с подпапками---------------------------------------------\n",
        "x = shutil.make_archive(new_path, 'zip', new_path)\n",
        "\n",
        "size_bytes = os.path.getsize(f'{perem_eng}.zip')\n",
        "size_mb = size_bytes / (1024 * 1024)\n",
        "\n",
        "print(f'\\nZip получен по пути {x}')\n",
        "print(f\"\\nЗазипованный файл весит {size_mb:.2f} MB.\")\n",
        "print()\n",
        "\n",
        "from google.colab import files\n",
        "zip_path = os.path.join('/content', f'{perem_eng}.zip')\n",
        "if os.path.exists(zip_path):\n",
        "    files.download(zip_path)\n",
        "print(f\"Зазипованный файл скачался успешно\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "id": "dndCbNKlTA8N",
        "outputId": "528fbc57-552c-4302-f4a6-61696fb2dcaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Удалено 35 файлов\n",
            "\n",
            "В папке \"Yandex\" осталось 122 файлов\n",
            "\n",
            "Zip получен по пути /content/Yandex.zip\n",
            "\n",
            "Зазипованный файл весит 51.46 MB.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b8efe3c3-6e5d-43e7-a810-f5c66672908b\", \"Yandex.zip\", 53955719)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Зазипованный файл скачался успешно\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "78983B7eTA5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xz1K0zVJTA2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9HdVY8cHTAv2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}